Experiments were done mostly with the ShapeNet dataset \cite{shapenet}, which is a large dataset of 3D models. 
A random set of either 1024 point clouds were sampled and then upscaled to double or 2048 points.
We then compare use some evaluation metrics with the ground truth to see how well our method performs in a quantitative manner.
In the appendix we highlight the qualitative results of our method compared to other methods used in this paper.


\subsection{Evaluation}

We will eventually evaluate our model using the Chamfer distance and Hausdorff distance as they are common metrics used in point cloud upsampling, and try to compare to other parameter free works as well as deep learning based methods.
The Chamfer distance is a measure of how different 2 shapes are and is defined as the following:

$$ C(P, Q) = \dfrac{1}{|P|} \sum\limits_{p \in P} \min_{q \in Q} \norm{p - q}^2 +  \dfrac{1}{|Q|} \sum\limits_{q \in Q} \min_{p \in P} \norm{p - q}^2 \label{eq:chamfer}$$

The Hausdorff distance is a measure of how similar 2 sets are. It is defined as the following:

$$ H(A, B) = \max(h(A, B), h(B, A))\label{eq:hausdorff}$$

Where:

$$h(A, B) = \max_{a \in A} \min_{b \in B} \norm{a - b}$$

In terms of comparisons, we perform comparisons with MLS as a baseline for another non-deep learning based method, as well as with PU-GCN as a deep learning based method.
We also see how our method performs compared to other smoothing methods such as KNN and no smoothing, as well as different sampling methods such as random sampling and octree sampling.
This is to show that our choices in smoothing and sampling are the best in this context.

\subsection{Comparison with Non-Deep Methods}

We compared our results with MLS, which is a non-deep learning based method of upsampling point clouds using local surface fitting.

\begin{table}[H]
\centering
\begin{tabular}{ccc}
	& Chamfer distance $\times 10^3$ &\\
	\hline
	class & MLS & Ours \\
	\hline
	plane & \textbf{14.8} & 15.8 \\ 
	helmets & \textbf{26.7} & 29 \\
	cap & \textbf{22.4} & 24.4 \\  
	car & \textbf{28.6} & 31 \\  
	headset & {25.3} & \textbf{23.8}  \\
\end{tabular}
\caption{Comparison of our method with MLS with chamfer distance $\times 10^3$. Note that lower is better}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ccc}
	& Hausdorff distance $\times 10^3$ &\\
	\hline
	class & MLS & Ours \\
	\hline
	plane & 282.9 & \textbf{80.5} \\ 
helmets & 454.8 &  \textbf{182.2}\\
cap & \textbf{150.2} & 156.3 \\ 
car & 75.3 & \textbf{73.5} \\  
headset & 171.3 & \textbf{166.5} \\
\end{tabular}
\caption{Comparison of our method with MLS with Hausdorff distance $\times 10^3$. Note that lower is better}
\end{table}

In general, our method performed better than MLS in terms of the Hausdorff distance, but worse in terms of the Chamfer distance.
In some cases, our method performed better in both metrics, such as the headset class.
In terms of execution time, we found that our method was also slower than MLS, taking 0.5 seconds to run in total compared to 0.2 seconds for MLS.
Note however that the MLS implementation was written entirely in C++. Our method only implements the bilateral filter in C++ and the rest in Python. 
This difference in overhead may account for the difference in execution time.

A worse chamfer distance but better hausdorff distance indicates that our method is better at preserving global shape, but worse at preserving local shape.
It also implies that our method is better at preserving the overall structure of the point cloud, but worse at preserving the details, and also implies that our method is less sensitive to outliers than MLS.
This sensitivity to outliers for MLS can be reflected in the car example in the appendix, where the MLS method has few points that are very far from the shape, where as our method does not have this issue.

Overall depending on the task at hand, one may choose to use MLS or our method. If the task requires preserving the overall structure of the point cloud, our method is better. If the task requires preserving the details of the point cloud, MLS is better.

\subsection{Comparison of Other Smoothing Methods}

We compared our method with other smoothing methods, such as the bilateral filter and a K-nearest neighbors based method as well as no smoothing and just the octree sampling.

\begin{table}[H]
\centering
\begin{tabular}{cccc}
	&  & Chamfer distance $\times 10^3$ &\\
	\hline
	class & KNN & Bilateral & None\\
	\hline
	plane & {16.2} & \textbf{15.8} & {16} \\ 
	helmets & {29.3} & \textbf{29} & {29.8} \\
	cap & {24.5} & \textbf{24.4} & {25.5}\\  
	car & {31.3} & \textbf{31} & \textbf{31}\\  
	headset & {24.4} & \textbf{23.8} & {24.2} \\
\end{tabular}
\caption{Comparison of our method with MLS with chamfer distance $\times 10^3$. Note that lower is better}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{cccc}
	&  & Hausdorff distance $\times 10^3$ &\\
	\hline
	class & KNN & Bilateral & None\\
	\hline
	plane & 81.3 & {80.5} & \textbf{78.4} \\ 
	helmets & 186.2 &  {182.2} & \textbf{182.1}\\
	cap & \textbf{149.4} & {156.3} & {155.6}\\ 
	car & \textbf{72.1} & {73.5} & {74}\\  
	headset & 168.3 & \textbf{166.5} & \textbf{166.5}\\
\end{tabular}
\caption{Comparison of our method with MLS with Hausdorff distance $\times 10^3$. Note that lower is better}
\end{table}

In general, our method performed better than the KNN method in terms of both the Chamfer and Hausdorff distance, with the exception of the cap and car classes. 
Bilateral also performed better than no smoothing in terms of Chamfer distance, but worse in terms of the Hausdorff distance.

\subsection{Comparison of Different Sampling Methods}

In this subsection we compare our octree sampling method with random sampling, both cases using bilateral smoothing.

\begin{table}[H]
\centering
\begin{tabular}{ccc}
	& Chamfer distance $\times 10^3$ &\\
	\hline
	class & Random & Octree \\
	\hline
	plane & {49.1} & \textbf{15.8} \\ 
	helmets & {40} & \textbf{29} \\
	cap & {38.7} & \textbf{24.4} \\  
	car & {36.7} & \textbf{31} \\  
	headset & {46} & \textbf{23.8}  \\
\end{tabular}
\caption{Comparison of our method with random sampling with chamfer distance $\times 10^3$. Note that lower is better}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ccc}
	& Hausdorff distance $\times 10^3$ &\\
	\hline
	class & Random & Octree \\
	\hline
	plane & 86.2 & \textbf{80.5} \\
	helmets & \textbf{172.4} &  {182.2}\\
cap & \textbf{153.4} & {156.3} \\
car & {74.6} & \textbf{73.5} \\
headset & 178.3 & \textbf{166.5} \\
\end{tabular}
\caption{Comparison of our method with random sampling with Hausdorff distance $\times 10^3$. Note that lower is better}
\end{table}

Clearly, the octree sampling method outperforms the random sampling method in terms of Chamfer distance, but slightly better in terms of the Hausdorff distance.
This shows that using an octree to voxelize and add points nearby points is a better method than randomly sampling points.

\subsection{Comparison with Deep Methods}

We will compare our method with deep learning based method PU-GCN\cite{PU-GCN}.
First, an analysis of the computational cost of using PU-GCN will be done. 
When experimenting with an Nvidia 4090, we found that the PU-GCN took 10GB of memory during training.
We trained the PU-GCN model for 10 epochs, which took 2 hours to train.
The original paper trained the model for 100 epochs. 
This is a significant amount of time and memory, and is a disadvantage of the PU-GCN method.
In the PU-GCN paper they authors claimed a chamfer distance of $\sim 0.5 \times 10^3$ and a Hausdorff distance of $\sim 1 \times 10^3$.
This is much lower than our method, but the computational cost is much higher.
This is also much lower than what we found in our experiments but is likely due to the fact that we only trained for 10 epochs.
This comparison was done with the PU1K dataset. Each point cloud was a sample of 256 points and was upsampled to 1024 points. 

\begin{table}[H]
\centering
\begin{tabular}{ccc}
	& Hausdorff $\times 10^3$ &\\
	\hline
	class & PU-GCN & Ours \\
	\hline
	eight & \textbf{96.1} & {221.3} \\ 
	elephant &  \textbf{74.1}& {122.6} \\
	elk & {94.6} & \textbf{52} \\  
	fandisk & \textbf{86} & {169.3} \\  
	genus3 & \textbf{127.1} & {378.6}  \\
\end{tabular}
\caption{Comparison of our method with PU-GCN with chamfer distance $\times 10^3$. Note that lower is better}
\end{table}

Our method does much worse in terms of the Hausdorff distance.
However, the computational cost of our method is much lower than PU-GCN.


